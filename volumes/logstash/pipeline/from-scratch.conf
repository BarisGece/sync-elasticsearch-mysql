input {
  jdbc {
    jdbc_driver_library => "/usr/share/logstash/mysql-connector-java-8.0.23.jar"
    jdbc_driver_class => "com.mysql.cj.jdbc.Driver" # "com.mysql.jdbc.Driver" is deprecated
    jdbc_connection_string => "jdbc:mysql://${MYSQL_PATH}:3306/${MYSQL_DATABASE}?zeroDateTimeBehavior=convertToNull"
    jdbc_user => "${MYSQL_USER}"
    jdbc_password => "${MYSQL_PASSWORD}"
    jdbc_paging_enabled => true # This will cause a sql statement to be broken up into multiple queries. Default: false
    jdbc_page_size => 100000 # JDBC page size. Default value is 100000
    tracking_column => "unix_ts_in_secs"
    use_column_value => true
    tracking_column_type => "numeric"
    sql_log_level => "debug"  # Set Logstash logging level as this
    clean_run => true # Set to true for indexing from scratch
    record_last_run => false
    statement_filepath => "/usr/share/logstash/config/queries/from-scratch.sql"
  }
}

filter {
  mutate {
    add_field => { "l_name" => "%{name}" }
    copy => { "team_id" => "[@metadata][_id]"}
    remove_field => ["team_id", "@version", "unix_ts_in_secs"]
  }

  mutate {
    lowercase => ["l_name"]
  }
}

output {
  # stdout { codec => rubydebug { metadata => true } }
  elasticsearch {
    #hosts => ["${ELASTIC_HOST}"]
    hosts => ["http://elasticsearch:9200"]
    index => "teams"
    user => "elastic"
    password => "${ELASTIC_PASSWORD}"
    ilm_enabled => false # Disabled for AWS ElasticSearch
    action => "update"
    document_id => "%{[@metadata][_id]}"
    doc_as_upsert => true
    template => "/usr/share/logstash/template/teams.json"
    template_name => "teams"
    template_overwrite => "false"
  }
}
